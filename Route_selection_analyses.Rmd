---
title: "Appendix: Route selection analyses"
author:
- Hester Br√∏nnvik  
- Department of Migration, Max Planck Institute of Animal Behavior, Radolfzell, Germany  
- Department of Biology, University of Konstanz, Konstanz, Germany
output: 
  pdf_document:
    highlight: haddock
---

## Performing analyses using Step Selection Functions

Step-selection functions model animal movement as a series of discrete steps between consecutive locations. Each observed step is matched with alternative steps, which allows comparisons of the observed locations to alternative, available ones. Here, we generate these alternative steps and available locations and compare the available airflow. 

Step-selections functions define availability with three criteria: the previous location, the time between consecutive locations, and the characteristics of an individual's movement. First, we have to clean the data and process them into tracks. Then, we have to re-sample them so that there are consistent time intervals between locations. Finally, we have to extract the step lengths and turn angles for each individual and generate distributions of what is possible. 

Once we have generated the available locations, we can label them with environmental information using Movebank's Env-DATA service. The last step is to ask how these environmental factors predict space use, which we do by fitting conditional logistic regressions. 

### 1. Prepare the environment

The first step is to make sure that we have all of the packages and functions that we need. Here, we will use the "amt" package to make and process our tracks and the "survival" package to model our data. We need several additional packages for formatting, mapping, and ease.

----
```{r libraries, message=FALSE, warning=FALSE, results='hide'}
#set a working directory and load required packages
setwd("C:/Users/Tess Bronnvik/Desktop/Br-nnvik_honey_buzzard_ssf")
mypath <- paste0(getwd(),"/")
ssf_packs <- c("lubridate", "amt", "move", "mapview", "ggpubr", "survival",
               "sjPlot", "MASS", "modelr", "mosaic", "caret", "tidyverse")
lapply(ssf_packs, require, character.only = TRUE)

#import required functions
NCEP.loxodrome.na <- function (lat1, lat2, lon1, lon2) {
  deg2rad <- pi/180
  acot <- function(x) {
    return(atan(1/x))
  }
  lat1 <- deg2rad * lat1
  lat2 <- deg2rad * lat2
  lon1 <- deg2rad * lon1
  lon2 <- deg2rad * lon2
  deltaLon <- lon2 - lon1
  pi4 <- pi/4
  Sig1 <- log(tan(pi4 + lat1/2))
  Sig2 <- log(tan(pi4 + lat2/2))
  deltaSig <- Sig2 - Sig1
  if (deltaLon == 0 && deltaSig > 0) {
    head <- 0
  }
  else if (deltaLon == 0 && deltaSig < 0) {
    head <- 180
  }
  else if (deltaSig == 0 && deltaLon > 0) {
    head <- 90
  }
  else if (deltaSig == 0 && deltaLon < 0) {
    head <- 270
  }
  else if (deltaSig < 0 && deltaLon < 0) {
    head <- acot(deltaSig/deltaLon) * 180/pi + 180
  }
  else if (deltaSig < 0 && deltaLon > 0) {
    head <- acot(deltaSig/deltaLon) * 180/pi + 180
  }
  else if (deltaSig > 0 && deltaLon > 0) {
    head <- acot(deltaSig/deltaLon) * 180/pi
  }
  else if (deltaSig > 0 && deltaLon < 0) {
    head <- acot(deltaSig/deltaLon) * 180/pi + 360
  }
  else {
    head <-NA}
  return(head)
}
# functions for calculating wind support from N/S and E/W wind and birds' directions of
# travel
source(paste0(mypath, "wind_support_Kami.R"))

```

----

Next, we set some criteria for building the tracks. The step number determines how many alternative steps are generated for each observed one. The tolerance determines how many minutes around a fix are allowed in order to consider it a next step (eg. a step is the distance traveled in 2 hours +/- 15 minutes). Finally, the CRS is the coordinate reference system, defining the map projection for the mk_track function. Here we have to inform R that our data are measured in degrees rather than in meters. 

----
```{r criteria}
# set criteria for tracks
stepNumber <- 100 # random steps. The number of steps used is arbitrary. The goal is to use
# enough to capture the actual distribution of available environmental conditions without
# running out of computational ability.
toleranceLength <- 15 # tolerance in minutes. We use 15 minutes because some of our steps
# are only 1 hour apart and we need to keep the timing precise.
wgs <- CRS("+proj=longlat +datum=WGS84 +no_defs") # map projection

```

----

### 2. Prepare data for Movebank's environmental data annotation

In order to get the data in the right format, we select only the reads from autumn migrations and clean them. We need to remove any duplicated reads and any times when the birds were not moving. We also need to map the data to see whether there are any incomplete records, and to remove those records if we find any. Then we need to separate the data so that individuals that were sampled at different rates are analyzed separately.

----
```{r data}
# retrieve the full data set
full_data <- read.csv(paste0(mypath,"original_data/full_buzz_set.csv"),
                      stringsAsFactors = F, header = T)

# reformat the time stamps
full_data$timestamp <- as.POSIXct(strptime(full_data$dt, format = "%Y-%m-%d %H:%M:%S"),
                                  tz = "UTC")

# select the autumn migrations
all_autumns <- full_data[grep("autumn", full_data$phase),]

# find and remove duplicate observations
doubles <- all_autumns %>% dplyr::select(long, lat, name, timestamp) %>%
  duplicated
sum(doubles) # 59 duplicates
all_autumns <- all_autumns[doubles != TRUE,]

# get the 204 reads when the birds did not move and remove them from the data
resting_reads <- read.csv(paste0(mypath,"resting.csv"), stringsAsFactors = F)
all_autumns <- all_autumns %>% mutate(id_ts = paste(name,timestamp, sep="_")) %>% 
  filter(!id_ts %in% resting_reads$id_ts)

# order all the data by timestamp
all_autumns <- all_autumns %>%
  arrange(timestamp)

# create a unique identifier for each migratory journey
all_autumns$id_year <- paste(all_autumns$name, all_autumns$yr, sep="_")

# make simple plots of lat/long to check for outliers
ggplot(all_autumns, aes(x=long, y=lat, color=as.factor(name))) + geom_point() +
  theme(legend.position = "none")
ggplot(all_autumns, aes(x=long, y=lat)) + geom_point()+ facet_wrap(~name, scales="free")


# make a map of individual tracks
data_sp <- all_autumns # store the data, then create a spatial object for plotting
coordinates(data_sp) <- ~ long + lat # set coordinates
proj4string(data_sp) <- wgs # set projection
mapView(data_sp, zcol = "id_year", burst = F, cex = 3, color = rainbow) # plot on a map

# we don't know the amount of experience that birds tagged as adults have
# collect the journeys for which experience can't be safely guessed
unk_age <- c("Annika_2011", "Annika_2012", "Annika_2013", "Jouko_2011",  "Jouko_2012",
             "Jouko_2013", "Mikko_2011", "Mikko_2012", "Mikko_2013",  "Paivi_2013",
             "Paivi_2014", "Paivi_2015", "Tiina_2011",  "Tiina_2012", "Tiina_2013")

# remove birds of unknown age with fewer than 4 tracked autumn migrations
all_autumns <- all_autumns %>% filter(#name != "Aarne" &
                         #name != "Jari" &
                         #name != "Johannes" &
                         #name != "Kari" & 
                 # disregard the data from early migrations by birds of unknown age
                        !id_year %in% unk_age &
                 # some individuals do not transmit their first trip
                         name != "Emma" &
                         name != "Lisa" &
                         name != "Miikka" &
                         name != "Per" &
                         name != "Sven" &
                 # others are lost en route later in life and must also be removed
                         id_year != "Annika_2015" & 
                         id_year != "Jouko_2015" & 
                         id_year != "Paivi_2017" &
                         id_year != "Senta_2017")

# because the loggers have different average sampling rates, 
# the data need to be re-sampled at different rates. Label them with these rates 
# in minutes so that each can be processed individually.
all_autumns$sample_rate <- NA

# the individuals with 1 hour sampling rates
all_autumns$sample_rate[which(all_autumns$name == "Anni" |
                              all_autumns$name == "Jari" |
                              all_autumns$name == "Johannes")] <- 60

# the individuals with 2 hour sampling rates
all_autumns$sample_rate[which(all_autumns$name == "Edit" | all_autumns$name == "Julia" |
                              all_autumns$name == "Matti"| all_autumns$name == "Ulla" |
                              all_autumns$name == "Aida" | all_autumns$name == "Ella" |
                              all_autumns$name == "Heidi" | all_autumns$name == "Kirsi" |
                              all_autumns$name == "Gilda" |
                              all_autumns$name == "Aarne" |
                              all_autumns$name == "Valentin")] <- 120

# the individuals with 3 hour sampling rates
all_autumns$sample_rate[which(all_autumns$name == "Mohammed")] <- 180

# the individuals with 4 hour sampling rates
all_autumns$sample_rate[which(all_autumns$name == "Annika" | all_autumns$name == "Jaana" |
                              all_autumns$name == "Kari"|
                              all_autumns$name == "Lars"| all_autumns$name == "Piff"| 
                              all_autumns$name == "Puff" | all_autumns$name == "Roosa"|
                              all_autumns$name == "Senta"| all_autumns$name == "Tor" |
                              all_autumns$name == "Tiina" |all_autumns$name == "Jouko" |
                              all_autumns$name == "Mikko" |all_autumns$name == "Paivi" |
                              all_autumns$name == "Hans" | all_autumns$name == "Venus" |
                              all_autumns$name == "Rudolf")] <- 240
```

----

Using the amt package, we can create track objects. These are then re-sampled to a given step length. Steps have to be of consistent times because the parameters of the models are scale dependent and will vary with $\Delta$t. By re-sampling, we burst the tracks. A burst is a track segment containing fixes separated by the given step length. We can then create randomized locations as part of these bursts.

----
```{r build-tracks, message=FALSE, warning=FALSE, results='hide'}
# build tracks at each rate

autumn_track <- data.frame()

for (i in sort(unique(all_autumns$sample_rate))) {
  # select data of the given sampling rate
  temp_rdata <- all_autumns[which(all_autumns$sample_rate == i),]
  for (j in sort(unique(temp_rdata$id_year))) {
  print(j)
  # select data for the given track
  temp_idata <- temp_rdata[which(temp_rdata$id_year == j),]
  # make the track  
  trk <- mk_track(temp_idata,.x=long, .y=lat, .t=timestamp, id = name, crs = wgs)
  # resample to a consistent time between steps
  trk <- track_resample(trk, rate = minutes(i), tolerance = minutes(toleranceLength))
  # remove bursts with fewer than three re-locations so that turn angle can be calculated
  trk <- filter_min_n_burst(trk, 3)
  # convert track to a step representation, and calculate sl_, direction, and ta_
  burst <- steps_by_burst(trk, keep_cols = "start")#, lonlat = T)
  # create random steps using fitted gamma and von Mises distributions (as suggested by
  # previous studies) and append
  rnd_stps <- burst %>% random_steps(n_control = stepNumber)
  # save
  autumn_track <- rbind(autumn_track, rnd_stps)
  # and signal
  print(paste0("Successfully created random steps for track ", j, "."))
  }
}

# compare the step lengths and turn angles for observed and random steps
obs_sl <- autumn_track[autumn_track$case_ == TRUE,] %>% 
  ggplot(aes(sl_, fill = factor(id))) + ggtitle("Observed") + 
  geom_density(alpha = 0.4) + 
  labs(x = "Step length (degrees)", y = "Density") +
  theme_minimal()
obs_ta <- autumn_track[autumn_track$case_ == TRUE,] %>% 
  ggplot(aes(ta_, fill = factor(id))) + ggtitle("Observed") +
  geom_density(alpha = 0.4) + 
  labs(x = "Turn angle (radians)", y = "Density") + 
  theme_minimal()
rand_sl <- autumn_track[autumn_track$case_ == FALSE,] %>%  
  ggplot(aes(sl_, fill = factor(id))) + ggtitle("Alternative") + 
  geom_density(alpha = 0.4) + 
  labs(x = "Step length (degrees)", y = "Density") +
  theme_minimal()
rand_ta <- autumn_track[autumn_track$case_ == FALSE,] %>% 
  ggplot(aes(ta_, fill = factor(id))) + ggtitle("Alternative") +
  geom_density(alpha = 0.4) + 
  labs(x = "Turn angle (radians)", y = "Density") + 
  theme_minimal()
ggarrange(obs_sl, rand_sl, obs_ta, rand_ta, ncol=2, nrow=2, legend = "none")

```

----

In order to calculate the wind support component of the north/south and east/west winds that we will get from Movebank annotation, we need to calculate the direction of movement (loxodrome angle of travel) between locations. This is best done before annotation to avoid truncation of smaller values in the locations during file export.

----
```{r headings, eval=FALSE}
# calculate directions of movement between locations for each step
autumn_track <- autumn_track %>% 
  rowwise() %>% 
  mutate(heading = NCEP.loxodrome.na(y1_, y2_, x1_, x2_)) %>% 
  ungroup()

```

----

Now we only need to conform to column name and file size specifications, then export the prepared data and send it to the Env-DATA service.

----
```{r export, eval=FALSE}
# select the end point for each step
names(autumn_track)[c(2,4)] <-c("location-long", "location-lat") 
# select the start time for each step
autumn_track$timestamp <- autumn_track$t1_ 
autumn_track$timestamp <- paste0(autumn_track$timestamp,".000" )

# export each file for Movebank annotation
write.csv(autumn_track, paste0("HB_", Sys.Date(), ".csv"), row.names = FALSE)
```

----

### 3. Prepare data for modeling

Step-selection analyses use conditional logistic regressions to predict whether a location is used or unused based on environmental conditions. Once we have our files returned from Movebank's Env-DATA service, we can process the annotated data for modeling. Using the directions we calculated above, we can derive wind support (m/s) from the wind data. Our second variable of interest is the planetary boundary layer height (m), which proxies uplift and subsidence. The boundary layer is higher where rising air pushes it upward. Our third variable is the interaction between wind support and uplift because we expect that birds select routes that maximize both.

----
```{r import, results='hide', message=FALSE, warning=FALSE, eval=F}
annotated_data <- read.csv(paste0(mypath,
                           "annotations/HB_2022-04-29-526598093103552488.csv"), 
                           stringsAsFactors = F, header = T)  %>%
  # rename the columns to be more useful
  rename(lon1 = x1_,
         lat1 = y1_,
         lon2 = location.long,
         lat2 = location.lat,
         u_wind = ECMWF.ERA5.PL.U.Wind,
         v_wind = ECMWF.ERA5.PL.V.Wind,
         vertical_pressure = ECMWF.ERA5.PL.Pressure.Vertical.Velocity,
         BLH = ECMWF.ERA5.SL.Boundary.Layer.Height) %>% 
  # then reformat the time and logical, and calculate wind support
  mutate(timestamp = as.POSIXct(strptime(timestamp, format = "%Y-%m-%d %H:%M:%S"),
                                tz = "UTC"),
         year = format(as.POSIXct(timestamp,format="%Y-%m-%d %H:%M:%S"),"%Y"),
         id_year = paste(id,year,sep="_"),
         case_ = as.numeric(case_),
         tail = wind_support(u_wind, v_wind, heading))


```
```{r rm_birds_silently, echo=F, warning=F, message=F, error=F}
load(paste0(mypath,"annotated_data_blh.RData"))


# we don't know the amount of experience that birds tagged as adults have
# collect the journeys for which experience can't be safely guessed
unk_age <- c("Annika_2011", "Annika_2012", "Annika_2013", "Jouko_2011",  "Jouko_2012",
             "Jouko_2013", "Mikko_2011", "Mikko_2012", "Mikko_2013",  "Paivi_2013",
             "Paivi_2014", "Paivi_2015", "Tiina_2011",  "Tiina_2012", "Tiina_2013")

# remove birds of unknown age with fewer than 4 tracked autumn migrations
annotated_data <- annotated_data %>% filter(id != "Aarne" &
                         id != "Jari" &
                         id != "Johannes" &
                         id != "Kari" & 
                         !id_year %in% unk_age &
                 # some individuals do not survive their first trip
                         id != "Emma" &
                         id != "Lisa" &
                         id != "Miikka" &
                         id != "Per" &
                         id != "Sven" &
                 # others died en route later in life and must also be removed
                         id_year != "Annika_2015" & 
                         id_year != "Jouko_2015" & 
                         id_year != "Paivi_2017" &
                         id_year != "Senta_2017")

# slice the annotated 150 steps to 101
annotated_data <- annotated_data %>%
  group_by(id_year, burst_, step_id_) %>% 
  slice(1:101) %>% 
  ungroup()

```

```{r environment, message=FALSE, warning=FALSE, results='hide'}

# label the data with life stage and migration information
annotated_data$stage <- NA
annotated_data$stage[which(annotated_data$id == "Annika" |
                             annotated_data$id == "Jouko" |
                             annotated_data$id == "Mikko" |
                             annotated_data$id == "Paivi" |
                             annotated_data$id == "Tiina" )] <- "adult"
annotated_data$stage[which(is.na(annotated_data$stage))] <- "juvenile"
                                
annotated_data$migration <- NA # create an empty column to store values
annotated_data$migration[which(annotated_data$id_year == "Jaana_2013" | # add year 2 info
                                 annotated_data$id_year == "Lars_2013" | 
                                 annotated_data$id_year == "Mohammed_2018" | 
                                 annotated_data$id_year == "Senta_2015" | 
                                 annotated_data$id_year == "Valentin_2015")] <- "2"

annotated_data$migration[which(annotated_data$id_year == "Jaana_2014" | # add year 3 info
                                 annotated_data$id_year == "Lars_2014" | 
                                 annotated_data$id_year == "Mohammed_2019" |
                                 annotated_data$id_year == "Senta_2016")] <- "3"

annotated_data$migration[which(annotated_data$id_year == "Lars_2015" | # add year 4 info
                                 annotated_data$id_year == "Mohammed_2020")] <- "4"

annotated_data$migration[which(annotated_data$id_year == "Annika_2014" | # add year 5 info
                            annotated_data$id_year == "Jouko_2014" |
                            annotated_data$id_year == "Mikko_2014" | 
                            annotated_data$id_year == "Mikko_2015" |
                            annotated_data$id_year == "Paivi_2016" |
                            annotated_data$id_year == "Tiina_2014" |
                            annotated_data$id_year == "Tiina_2015")] <- "Adult" 
annotated_data$migration[which(is.na(annotated_data$migration))] <- "1" # everything else is year 1

annotated_data$category <- NA
annotated_data$category[which(annotated_data$migration == 1)] <- "First year"
annotated_data$category[which(annotated_data$migration == "Adult")] <- "Adult"
annotated_data$category[which(annotated_data$migration %in% c(2,3,4))] <- "Intermediate"
annotated_data$category <- factor(annotated_data$category, levels = c("Adult", "Intermediate", "First year"))

```
---

```{r, densityPlots}
age_col <- c("#ACC6FF", "#7A9EFF", "#3A6FFF", "#0044FF", "#0032BD")
annotated_data$migration <- factor(annotated_data$migration, levels = c("Adult", 4, 3, 2, 1))

ws <- annotated_data[annotated_data$case_ == TRUE,] %>% 
    ggplot(aes(tail, migration, fill = migration, color = NULL)) + 
    geom_density_ridges2(alpha = 0.8, quantile_lines=TRUE,
                      quantile_fun=function(x,...)mean(x)) + 
    labs(x = "Wind support (m/s)", y = "", fill = "Migration")+
    scale_fill_manual(values=age_col)+
    coord_cartesian(expand = F)+
    theme_minimal() +
    theme(legend.position = "none", text = element_text(size=15), 
        axis.text.x = element_text(colour = "black"),
        axis.text.y = element_text(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        strip.text.x = element_text(size = 20),
        axis.line = element_line(colour = "black"),
        axis.ticks = element_line(colour = "black"))
vv <- annotated_data[annotated_data$case_ == TRUE,] %>% 
    ggplot(aes(vertical_pressure, migration, fill = migration)) + 
    geom_density_ridges2(alpha = 0.8,quantile_lines=TRUE,
                      quantile_fun=function(x,...)mean(x)) + 
    labs(x = "Vertical velocity (Pa/s)", y = "Density", fill = "Migration")+
    scale_fill_manual(values=age_col)+
    coord_cartesian(expand = F)+
    theme_minimal() +
    theme(legend.position = "none", text = element_text(size=15), 
        axis.text.x = element_text(colour = "black"),
        axis.text.y = element_text(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        strip.text.x = element_text(size = 20),
        axis.line = element_line(colour = "black"),
        axis.ticks = element_line(colour = "black"))
bl <- annotated_data[annotated_data$case_ == TRUE,] %>% 
    ggplot(aes(BLH, migration, fill = migration)) + 
    geom_density_ridges2(alpha = 0.8, quantile_lines=TRUE,
                      quantile_fun=function(x,...)mean(x)) + 
    labs(x = "Boundary layer height (m)", y = "", fill = "Migration")+
    scale_fill_manual(values=age_col)+
    coord_cartesian(expand = F)+
    theme_minimal() +
    theme(legend.position = "none", text = element_text(size=15), 
        axis.text.x = element_text(colour = "black"),
        axis.text.y = element_text(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        strip.text.x = element_text(size = 20),
        axis.line = element_line(colour = "black"),
        axis.ticks = element_line(colour = "black"))
ggarrange(ws, vv, bl, ncol = 1, nrow = 3, common.legend = T, legend = "bottom")
```

----

As a last step we standardize our predictors across the whole data set. This means that all of our data have the same mean and variance, and therefore allows us to compare the coefficients of different models. We use "scale" to divide each value by its column's standard deviation and subtract the mean.

----
```{r standardize}
#standardize the predictors
annotated_data <- annotated_data %>% 
  mutate(scaled_tail = as.numeric(scale(tail, center = T, scale = T)),
         scaled_blh = as.numeric(scale(BLH, center = T, scale = T)),
         scaled_angle = as.numeric(scale(cos(ta_), center = T, scale = T)),
         scaled_step = as.numeric(scale(log(sl_), center = T, scale = T)))

```
---

### 4. Build the model

We need to model each individual in each year separately, which means fitting as many models as there are tracks.

---
```{r modeling, results='hide'}
# write the model function
modelTU <- function(df) {
  clogit(case_ ~ scaled_tail + scaled_blh + scaled_step + scaled_angle + scaled_tail*scaled_blh +
           strata(step_id_), data = df)
}

#check how many true steps remain after filtering
print(annotated_data %>% group_by(id_year) %>% tally(case_ == 1), n= 41)

# fit separate models for each journey and collect the coefficients
SSF_results <- data.frame()

for (i in sort(unique(annotated_data$id_year))) {
  # select data for just one track
  data <- annotated_data[which(annotated_data$id_year == i),]
  # fit the model
  ssf_modelTU <- modelTU(data)
  # extract the coefficients
  temp <- data.frame(coefs = coef(ssf_modelTU))
  # extract the error
  temp$se <- as.numeric(coef(summary(ssf_modelTU))[, "se(coef)"])
  # add the track iD
  temp$id_year <- i
  # and the migration number
  temp$migration <- unique(data$migration)
  temp$category <- unique(data$category)
  # and save the information
  SSF_results <- rbind(SSF_results, temp)
  print(paste0("Extracted model coefficients for ", i, "."), quotes=F)
}

# add the variable names
SSF_results$variable <- rep(c("Wind support", "Boundary layer height", "ln_sl", "cos_ta", "Interaction"), times = nrow(SSF_results)/5)

```

----

### 5. Plot the model coefficients

We plot the coefficients and their error grouped by migration, along with box plots for each migration to show the median for all individuals.

---
```{r boxplots, message = F, warning=F, fig.height = 5, fig.width = 8}
# filter the SSF_results so as not to display the step length and turn angle coefficients
# (these factors of movement account for autocorrelation, but are not informing the study).
plot_data <- SSF_results %>% filter(variable != "ln_sl" & variable != "cos_ta")
# rename the variables and define the factor order to arrange the plot
plot_data$variable[grep("Boundary layer height", plot_data$variable)] <- "Uplift (BLH)"
plot_data$variable[grep("Interaction", plot_data$variable)] <- "Wind support X Uplift"
plot_data$variable <- factor(plot_data$variable, levels = c("Wind support", "Uplift (BLH)", "Wind support X Uplift"))

# make the figure
ggplot(plot_data, aes(x=category, y=coefs)) + 
  # plot a box plot without outliers (the points are added later)
  geom_boxplot(outlier.shape = NA, fill = "#DFF2F6", color = "#9FD9E5", lwd = 1, fatten = 1.5)+ 
  # and a second boxplot with no whiskers, no fill, and no median to cover 
  # the first plot's outlines
  geom_boxplot(aes(ymin = ..lower.., ymax = ..upper..), outlier.shape = NA, fill = NA, color = "#DFF2F6", lwd = 1.5, fatten = NULL) +
  # add error bars for the points using +/- standard error from SSF_results
  geom_errorbar(
    aes(ymin = coefs-se, ymax = coefs+se, group = id_year),
    position = position_dodge(0.8), width = 0, color = "grey60"
    )+
  # add the points for the coefficients
  geom_point(position = position_dodge(0.8), aes(group = id_year), size = 2)+
  # make a line at y = 0 for clarity
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  # orrect the axis labels
  labs(x = "Migration", y="Coefficient") +
  # remove background grid, make axis labels black, increase text size, and add axes
  theme_minimal() +
    facet_wrap(facets = "variable", nrow = 1, ncol = 3) +
    theme(legend.position = "none", text = element_text(size=15), 
        axis.text.x = element_text(colour = "black"),
        axis.text.y = element_text(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        strip.text.x = element_text(size = 20),
        axis.line = element_line(colour = "black"),
        axis.ticks = element_line(colour = "black"))

```


Figure 1: The importance of wind support, uplift (boundary layer height), and their interaction to route selection over migrations. "First year" denotes just the juveniles' first autumn migrations, "Intermediate" includes the second, third, and fourth autumn migrations by those same birds. "Adult" refers to different birds that were tagged as adults. We fitted separate conditional logistic regressions for each track and extracted the coefficients for each predictor.


----

### 6. Validate the model

We have our result, but we do not know how well the model actually performed. We can perform cross-validation to test the predictive power of our model. We do this by splitting the data into two sets, training the model using the bulk of the data, and then asking how well it predicts the remaining data. 

---

```{r validation,warning=F}
# pare the data down
validation_data <- annotated_data %>% select(case_, id, year, burst_, step_id_, scaled_tail,
                                             scaled_blh, scaled_step, scaled_angle)  %>%
                                             na.omit()

# partition the data into 80% for training the model and 20% for testing it
# ensuring that observed and alternative steps are included in both sets (case_ 0 and 1)
partitioning <- validation_data$case_ %>%
  createDataPartition(p = 0.8, list = FALSE)

training_data <- validation_data[partitioning,]
testing_data <- validation_data[-partitioning,]

# fit the model to 80% of the data
validation_model <- clogit(case_ ~ scaled_tail + scaled_blh + scaled_step + scaled_angle +
                           scaled_tail*scaled_blh + strata(step_id_), data = training_data)

# extract the model predictions
predictions <- validation_model %>% predict(testing_data)

# calculate the RMSE and MAE
data.frame(RMSE = RMSE(predictions, testing_data$case_),
            MAE = MAE(predictions, testing_data$case_))

```

----

## Permutations and significance testing 

Because we have only one data set, we cannot compare our result to the results from similar systems: we have no other data sets of the same size, with the same sampling effort, and from populations with the same means and variances of the parameters of interest. However, we can generate alternative data sets that meet these requirements and that also meet a null expectation. Then, we can compare our observed effects to the effects in these alternative data. To do this, we repeatedly randomize our response variable with respect to our predictor variables, model these randomized data, and store the results. The outcome is a distribution of results that would occur if there were no relationship between the dependent and independent variables in our model. 

### 1. Perform data stream permutation and test for significance

Once we have modeled all of our observed data, we can run our randomizations. Unfortunately, this becomes a repetitive process because each migration must be modeled separately, the significance of each variable in each migration must be calculated separately, and each distribution of coefficients must be plotted separately. We create an empty data frame so that we have a place to store our results. Then, we use the "mosaic" package to re-sample our data set. We hold our predictors in the same positions relative to the identifying data for each individual and to the other predictors. We shuffle the dependent variable within each group, which we define as a step containing one observation and 100 random locations. Next, we fit our model to the randomized data, extract the coefficients, and save them in our data frame. We repeat this process as many times as we want, which we defined as "no.perm". 

```{r permutations, message=FALSE, warning=FALSE}

# set the number of permutations
# here it is equal to the number of used + available steps in each stratum, 
# i.e. all available permutations in a stratum
no.perm <- 101 
# make a new unique vector for each step and migration to serve as strata during permutation
annotated_data$id_year_step <- paste(annotated_data$id_year, annotated_data$step_id_, sep="_")
#create an empty data frame to store the coefficients in
random_model <- data.frame()
# create an empty vector that will act as a column in the data frame 
```
```{r permutations2, message=FALSE, warning=FALSE, results=F, echo=T, eval=F}
Category <- c()
# create a loop to go through each migration
for (j in sort(unique(annotated_data$category))) {
  # select the data from the given migration
  temp_data <- annotated_data[which(annotated_data$category == j),]
# then create a second loop to permute the data
  for (i in 1:no.perm) {
    # this is the workhorse, taking each migration and permuting within the strata
    # while holding the predictors variables in place so as not to break their relationships
    rando <- mosaic::resample(temp_data, replace = F, groups = id_year_step, 
                              shuffled = "case_", fixed = c("id","year","scaled_tail",
                              "scaled_blh","timestamp", "scaled_step", "scaled_angle",
                              "step_id_"))
    # model those permuted data sets
    temp_model <- modelTU(rando)
    # extract the coefficients from the model
    temp_coefs <- summary(temp_model)$coefficients
    # store the coefficients in the data frame
    random_model <- rbind(random_model, temp_coefs)
    # add three more migration labels to identify the three coefficients
    Category <- c(Category,j,j,j,j,j)
    # and finally we let ourselves know we have done this
    print(paste0("Completed permutation ", i, "."))
  }
  print(paste0("Completed permutation of ", j, " migration."))
}
colnames(random_model)[1] <- "Estimate"
# add each migration label to the coefficients
random_model$Category <- Category
random_model$Category <- factor(random_model$Category, levels=c("First year", "Intermediate","Adult"))
```
```{r hidden2, echo=FALSE, warning=F, error=F}
#load("C:/Users/Tess Bronnvik/Desktop/Br-nnvik_honey_buzzard_ssf/random_model_Jan20.RData")
load("categories_blh_17.5.22.RData")
colnames(random_model)[6] <- "Category"
```
```{r model}
head(random_model[,1:5])
```

---

Once we have aggregated all of the coefficients of the permutations for each migration, we compare the observed coefficients to the randomized ones to test for significance.
```{r rand_loads, echo=F, message=F,warning=F, eval=F}
## 27.04.2022
# 17 perm done twice
#load(file="rando_270422.RData")
## 06.05.2022
# scaled Delta T model with sl_ and ta_
#save(random_model, file = "delT_random.RData")
# scaled BLH model with sl_ and ta_ and without sea crossings
#save(random_model, file = "blh_scrm_random.RData")
# scaled BLH model with sl_ and ta_ and sea crossings
#save(random_model, file = "blh_sc_11.5.22.RData")
```

---
```{r significance}

## First extract the coefficients of the models for the observed data
# fit the same model as above, but now for each migration rather than each individual track

clogit_results <- data.frame()

for (i in sort(unique(annotated_data$category))) {
  data <- annotated_data[which(annotated_data$category == i),]
  ssf_modelTU <- modelTU(data)
  temp <- data.frame(coefs = coef(ssf_modelTU))
  temp$se <- as.numeric(coef(summary(ssf_modelTU))[, "se(coef)"])
  temp$category <- i
  clogit_results <- rbind(clogit_results, temp)
  print(paste0("Extracted model coefficients for ", i, "."), quotes=F)
}

clogit_results$variable <- rep(c("Wind support", "Boundary layer height", 
                                 "ln_sl", "cos_ta", "Interaction"), 
                                 times = nrow(clogit_results)/5)

print(clogit_results)


# rename the interaction term in the random coefficients to keep it unique
row.names(random_model) <- sub("scaled_tail:scaled_blh", "Interaction",
                               row.names(random_model))
# rename the other terms to match clogit_results
row.names(random_model) <- sub("scaled_blh", "Boundary layer height",
                               row.names(random_model))
row.names(random_model) <- sub("scaled_tail", "Wind support",
                               row.names(random_model))
# rename the sl_ and ta_ to prevent errors in escaping parentheses
row.names(random_model) <- sub("scaled_angle", "ta_", row.names(random_model))
row.names(random_model) <- sub("scaled_step", "sl_", row.names(random_model))


# create a data frame to store the significance values in
significances <- data.frame()

# then calculate the significance of the difference between each set of permuted coefficients 
# and the observed 
for (i in sort(unique(annotated_data$category))) {
  # select the coefficients for migration i
  temp_observed <- clogit_results[which(clogit_results$category == i),]
  # and the coefficients from permuted data of migration i
  temp_random <- random_model[which(random_model$Category == i),]
  for (j in sort(unique(clogit_results$variable))) {
    # select the coefficient that is also variable j
    temp_obs <- temp_observed[which(temp_observed$variable == j),]
    # and the permuted coefficients of j
    temp_rand <- temp_random[grepl(j, row.names(temp_random)),]
    # count up the permuted coefficients with a more extreme value than the observed and 
    # divide by the number of coefficients to get the proportion
    significance <- sum(abs(temp_obs$coefs) < abs(temp_rand$Estimate))/no.perm
    temp_sig <- c(i, j, significance)
    significances <- rbind(significances, temp_sig)
  }
}

colnames(significances) <- c("Migration", "Variable", "Significance")

significances %>% arrange(Variable)
```

---

## Plot the supplemental materials

Plot effects of wind support and uplift for each individual over time.

```{r supplots}
# select colors that are distinct
id_colors <- c("#542344","#1478A3","#00A354","#FFBF00","#F34213")
# create a vector identifying the five birds that transmitted multiple journeys
SSF_results <- separate(SSF_results, col = "id_year", c("id", "year"), remove = F)
retransmitted <- c("Jaana", "Senta", "Valentin", "Mohammed", "Lars")
# arrange the migrations in order
SSF_results$migration <- factor(SSF_results$migration, levels = c("1", "2", "3", "4", "Adult"))
# build a function for plotting 
SSF_plot <- function(data, y_axis, x_axis, var) {
  # select the data to plot
  plot_data = data %>% filter(variable == var)
  # plot them
  ggplot(data, aes(x={{x_axis}}, y={{y_axis}})) + 
  # a box plot with all the data
  geom_boxplot(outlier.shape = NA, color = "grey30", linetype="dashed")+ 
  # a second box plot without whiskers
  stat_boxplot(aes(ymin = ..lower.., ymax = ..upper..), outlier.shape = NA, 
               fill = "grey80", lwd = 0.75, fatten = 1.2) +
  # the points for individuals that died
  geom_jitter(data = plot_data %>% filter(!id %in% retransmitted), aes(), size=2, width=0.3, color = "grey30") +
  # the points for individuals with multiple migrations
  geom_point(data = plot_data %>% filter(id %in% retransmitted), aes(x={{x_axis}},
             y={{y_axis}}, fill = id), shape = 21, size = 3, colour = "transparent") +
  # the lines connecting the repeated journeys
  geom_line(data = plot_data %>% filter(id %in% retransmitted), aes(group = id, color =id), size = 1.2) +
  # color the points
  scale_fill_manual(values = id_colors) + 
  # color the lines
  scale_color_manual(values = id_colors) + 
  # make a line at y = 0 for clarity
  geom_hline(yintercept = 0) +
  # remove the labels
  labs(x = "", y="")+
  # remove background grid, make axis labels black, and increase text size
  theme_classic() +
    theme(legend.position = "none", text = element_text(size=15), 
        axis.text.x = element_text(colour = "black"),
        axis.text.y = element_text(colour = "black"))
}

support_plot <- SSF_plot(SSF_results, coefs, migration, var = "Wind support") + ggtitle("Wind support")
uplift_plot <- SSF_plot(SSF_results, coefs, migration, var = "Boundary layer height") + ggtitle("Uplift (BLH)")
interaction_plot <- SSF_plot(SSF_results, coefs, migration, var = "Interaction") + ggtitle("Wind support x Uplift")

figure1 <- ggarrange(support_plot, uplift_plot, interaction_plot, ncol = 3, legend = "none")

annotate_figure(figure1, left = text_grob("Coefficient", rot = 90, size = 20),
                bottom = text_grob("Migration", size = 20))
```


Plot the distributions of coefficients derived from randomized data along with the observed coefficients.

---
```{r plots, message=FALSE, warning=FALSE, results='hide', out.extra='angle=90', fig.dim=c(6,6)}
# Comparisons of the observed coefficients (red) and the distribution of coefficients from
# randomized data (black) in each migratory journey (1 to 5 or more) and for each predictor.

# order the migrations
trip <- unique(clogit_results$category)

# write a function to draw the plots
null_plot <- function(data, x_axis, journey, var) {
  # select the variable to plot
  var = rownames(data)[1]
  # select the migration to plot
  ggplot(data %>% filter(Category == {{journey}}), aes({{x_axis}})) + 
    # draw a histogram of the coefficients from the randomized data
    geom_histogram(fill="black", bins = 25) +
    # draw a line to indicate the observed coefficient
    geom_vline(aes(xintercept = clogit_results$coefs[which(clogit_results$category == {{journey}} & clogit_results$variable == {{var}})]), 
               color="red", size = 1)+ 
    # add axis labels
    labs(y=NULL, x= journey)+
    # clean up the plotting area
    theme_classic() + 
    # increase contrast
    theme(legend.position = "none", text = element_text(size=15), 
          axis.text = element_text(colour = "black"))+
    # if the plot is of the interaction, add an x-axis label for migration
    if(var == "Interaction"){theme(axis.title = element_text(
      size = 15))}else{theme(axis.title = element_blank())}
}

# separate the data by variable
ct <- filter(random_model, grepl("Wind support", rownames(random_model)))
cl <- filter(random_model, grepl("Boundary layer height", rownames(random_model)))
ci <- filter(random_model, grepl("Interaction", rownames(random_model)))

# create the plots
first_tail <- null_plot(ct, Estimate, trip[1]) + ggtitle("Wind support")
second_tail <- null_plot(ct, Estimate, trip[2]) + ggtitle(" ")
third_tail <- null_plot(ct, Estimate, trip[3]) + ggtitle(" ")

first_lift <- null_plot(cl, Estimate, trip[1]) + ggtitle("Uplift (BLH)")
second_lift <- null_plot(cl, Estimate, trip[2]) + ggtitle(" ")
third_lift <- null_plot(cl, Estimate, trip[3]) + ggtitle(" ")

first_inter <- null_plot(ci, Estimate, trip[1]) + ggtitle("Wind x Uplift")
second_inter <- null_plot(ci, Estimate, trip[2]) + ggtitle(" ")
third_inter <- null_plot(ci, Estimate, trip[3]) + ggtitle(" ")

# combine the plots
permutations <- ggarrange(first_tail,second_tail,third_tail, 
          first_lift,second_lift,third_lift, 
          first_inter,second_inter,third_inter,
          ncol = 3, nrow=3)

# add common axes
permutations <- annotate_figure(permutations, left = text_grob("Frequency", rot = 90, size = 20),
                bottom = text_grob("Coefficient", size = 20))
permutations
```

---

To look at the permutations for all five migratory journeys separately:
```{r permutations3, message=FALSE, warning=FALSE}
#create an empty data frame to store the coefficients in
random_model <- data.frame()
# create an empty vector that will act as a column in the data frame 
```
```{r permutations4, message=FALSE, warning=FALSE, results=F, echo=T, eval=F}
Migration <- c()
# create a loop to go through each migration
for (j in sort(unique(annotated_data$migration))) {
  # select the data from the given migration
  temp_data <- annotated_data[which(annotated_data$migration == j),]
# then create a second loop to permute the data
  for (i in 1:no.perm) {
    # this is the workhorse, taking each migration and permuting within the strata
    rando <- mosaic::resample(temp_data, replace = F, groups = id_year_step, 
                              shuffled = "case_", fixed = c("id","year","scaled_tail",
                              "scaled_blh","timestamp", "scaled_step", "scaled_angle",
                              "step_id_"))
    # model those permuted data sets
    temp_model <- modelTU(rando)
    # extract the coefficients from the model
    temp_coefs <- summary(temp_model)$coefficients
    # store the coefficients in the data frame
    random_model <- rbind(random_model, temp_coefs)
    # add three more migration labels to identify the three coefficients
    Migration <- c(Migration,j,j,j,j,j)
    # and finally we let ourselves know we have done this
    print(paste0("Completed permutation ", i, "."))
  }
  print(paste0("Completed permutation of ", j, " migration."))
}
colnames(random_model)[1] <- "Estimate"
# add each migration label to the coefficients
random_model$Migration <- Migration
```

```{r hidden, echo=FALSE, warning=F, error=F}
#load("C:/Users/Tess Bronnvik/Desktop/Br-nnvik_honey_buzzard_ssf/random_model_Jan20.RData")
load("blh_sc_11.5.22.RData")
random_model$Migration[which(random_model$Migration == "first")] <- 1
random_model$Migration[which(random_model$Migration == "second")] <- 2
random_model$Migration[which(random_model$Migration == "third")] <- 3
random_model$Migration[which(random_model$Migration == "z_fourth")] <- 4
random_model$Migration[which(random_model$Migration == "adult")] <- "Adult"
random_model$Migration <- factor(random_model$Migration, levels=c(1,2,3,4,"Adult"))
# rename the interaction term in the random coefficients to keep it unique
row.names(random_model) <- sub("interaction", "scaled_tail:scaled_blh",
                               row.names(random_model))
```

```{r model2}

## First extract the coefficients of the models for the observed data
# fit the same model as above, but now for each migration rather than each individual track

clogit_results <- data.frame()

for (i in sort(unique(annotated_data$migration))) {
  data <- annotated_data[which(annotated_data$migration == i),]
  ssf_modelTU <- modelTU(data)
  temp <- data.frame(coefs = coef(ssf_modelTU))
  temp$se <- as.numeric(coef(summary(ssf_modelTU))[, "se(coef)"])
  temp$migration <- i
  clogit_results <- rbind(clogit_results, temp)
  print(paste0("Extracted model coefficients for ", i, "."), quotes=F)
}

clogit_results$variable <- rep(c("Wind support", "Boundary layer height", "ln_sl", "cos_ta", "Interaction"), times = nrow(clogit_results)/5)


# rename the interaction term in the random coefficients to keep it unique
row.names(random_model) <- sub("scaled_tail:scaled_blh", "Interaction",
                               row.names(random_model))
# rename the other terms to match clogit_results
row.names(random_model) <- sub("scaled_blh", "Boundary layer height",
                               row.names(random_model))
row.names(random_model) <- sub("scaled_tail", "Wind support",
                               row.names(random_model))
# rename the sl_ and ta_ to prevent errors in escaping parentheses
row.names(random_model) <- sub("scaled_angle", "ta_", row.names(random_model))
row.names(random_model) <- sub("scaled_step", "sl_", row.names(random_model))


# create a data frame to store the significance values in
significances <- data.frame()

# then calculate the significance of the difference between each set of permuted coefficients 
# and the observed 
for (i in sort(unique(annotated_data$migration))) {
  # select the coefficients for migration i
  temp_observed <- clogit_results[which(clogit_results$migration == i),]
  # and the coefficients from permuted data of migration i
  temp_random <- random_model[which(random_model$Migration == i),]
  for (j in sort(unique(clogit_results$variable))) {
    # select the coefficient that is also variable j
    temp_obs <- temp_observed[which(temp_observed$variable == j),]
    # and the permuted coefficients of j
    temp_rand <- temp_random[grepl(j, row.names(temp_random)),]
    # count up the permuted coefficients with a more extreme value than the observed and 
    # divide by the number of coefficients to get the proportion
    significance <- sum(abs(temp_obs$coefs) < abs(temp_rand$Estimate))/no.perm
    temp_sig <- c(i, j, significance)
    significances <- rbind(significances, temp_sig)
  }
}

colnames(significances) <- c("Migration", "Variable", "Significance")

significances %>% arrange(Variable, Migration)
```
```{r plots2, message=FALSE, warning=FALSE, results='hide', out.extra='angle=90', fig.dim=c(6,6)}
# Comparisons of the observed coefficients (red) and the distribution of coefficients from
# randomized data (black) in each migratory journey (1 to 5 or more) and for each predictor.

# order the migrations
trip <- sort(unique(clogit_results$migration))

# write a function to draw the plots
null_plot <- function(data, x_axis, journey, var) {
  # select the variable to plot
  var = rownames(data)[1]
  # select the migration to plot
  ggplot(data %>% filter(Migration == {{journey}}), aes({{x_axis}})) + 
    # draw a histogram of the coefficients from the randomized data
    geom_histogram(fill="black", bins = 25) +
    # draw a line to indicate the observed coefficient
    geom_vline(aes(xintercept = clogit_results$coefs[which(clogit_results$migration == {{journey}} & clogit_results$variable == {{var}})]), 
               color="red", size = 1)+ 
    # add axis labels
    labs(y=NULL, x= journey)+
    # clean up the plotting area
    theme_classic() + 
    # increase contrast
    theme(legend.position = "none", text = element_text(size=15), 
          axis.text = element_text(colour = "black"))+
    # if the plot is of the interaction, add an x-axis label for migration
    if(var == "Interaction"){theme(axis.title = element_text(
      size = 15))}else{theme(axis.title = element_blank())}
}

# separate the data by variable
ct <- filter(random_model, grepl("Wind support", rownames(random_model)))
cl <- filter(random_model, grepl("Boundary layer height", rownames(random_model)))
ci <- filter(random_model, grepl("Interaction", rownames(random_model)))

# create the plots
first_tail <- null_plot(ct, Estimate, journey = trip[1]) + ggtitle("Wind support")
second_tail <- null_plot(ct, Estimate, journey = trip[2]) + ggtitle(" ")
third_tail <- null_plot(ct, Estimate, journey = trip[3]) + ggtitle(" ")
fourth_tail <- null_plot(ct, Estimate, journey = trip[4]) + ggtitle(" ")
fifth_tail <- null_plot(ct, Estimate, journey = trip[5]) + ggtitle(" ")

first_lift <- null_plot(cl, Estimate, journey = trip[1]) + ggtitle("Uplift (BLH)")
second_lift <- null_plot(cl, Estimate, journey = trip[2]) + ggtitle(" ")
third_lift <- null_plot(cl, Estimate, journey = trip[3]) + ggtitle(" ")
fourth_lift <- null_plot(cl, Estimate, journey = trip[4]) + ggtitle(" ")
fifth_lift <- null_plot(cl, Estimate, journey = trip[5]) + ggtitle(" ")

first_inter <- null_plot(ci, Estimate, journey = trip[1]) + ggtitle("Wind x Uplift")
second_inter <- null_plot(ci, Estimate, journey = trip[2]) + ggtitle(" ")
third_inter <- null_plot(ci, Estimate, journey = trip[3]) + ggtitle(" ")
fourth_inter <- null_plot(ci, Estimate, journey = trip[4]) + ggtitle(" ")
fifth_inter <- null_plot(ci, Estimate, journey = trip[5]) + ggtitle(" ")

# combine the plots
all_15 <- ggarrange(first_tail,second_tail,third_tail,fourth_tail,fifth_tail, 
          first_lift,second_lift,third_lift,fourth_lift,fifth_lift, 
          first_inter,second_inter,third_inter,fourth_inter,fifth_inter,
          ncol = 5, nrow=3)

# add common axes
all_15 <- annotate_figure(all_15, left = text_grob("Frequency", rot = 90, size = 20),
                bottom = text_grob("Coefficient", size = 20))
all_15
```